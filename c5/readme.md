

## Model Architecture and Data Preprocessing

This section details the methodology employed for sign language word classification, encompassing data selection, keypoint extraction, normalization, augmentation, and the neural network architecture.

### Data Selection and Keypoint Extraction

The study focuses on classifying a subset of 10 distinct sign language words, selected from available datasets. The chosen classes correspond to the labels: 'sleep' (0162), 'silence' (0165), 'love' (0174), 'smoke' (0183), 'support' (0184), 'confused' (0234), 'worried' (0235), 'here' (0285), 'greeting' (0290), and 'thanks' (0293). Data from multiple source directories containing frame sequences for these classes were combined to form a unified dataset.

The primary input features are derived from hand keypoints extracted from video frames. We utilized the MediaPipe Hands solution [1] to detect and localize 21 3D landmarks for each hand present in a frame. For simplicity and focusing on shape and relative motion, only the 2D coordinates (x, y) of the landmarks from the first detected hand were used in this iteration. Keypoint extraction was performed independently on each frame of a sign sequence.

### Keypoint Normalization and Augmentation

Raw keypoint coordinates are sensitive to the hand's position, size, and orientation within the camera frame. To achieve invariance to these factors, a normalization procedure was applied to the extracted landmarks of each frame:

1.  **Centering:** All keypoint coordinates were translated relative to the wrist landmark (landmark index 0), effectively centering the hand representation.
2.  **Scaling:** The translated coordinates were then scaled based on a measure of hand size. The Euclidean distance between the wrist (landmark 0) and the metacarpophalangeal (MCP) joint of the middle finger (landmark 9) was used as the scaling factor. This normalizes the representation for variations in hand size and distance from the camera. Coordinates were divided by this distance. Frames where this distance was near-zero (indicating a potential detection error) were handled to avoid division by zero, typically resulting in a zero-feature vector for that frame.
3.  **Flattening:** The normalized 2D coordinates (x, y) for all 21 landmarks were flattened into a single feature vector of dimension 42 (`FEATURE_DIM = 21 * 2 = 42`) for each frame.

To improve model generalization and robustness, particularly given the potential for limited data, horizontal flip augmentation was applied during preprocessing. For sequences designated for the training set, a mirrored version was generated by applying the transformation `x_flipped = 1.0 - x_original` to the horizontal coordinate of each keypoint *before* the normalization steps. The original sequences were always retained. Test set sequences were *not* augmented.

Finally, each sequence of feature vectors was either padded with zero vectors or truncated to a fixed maximum sequence length (`MAX_SEQ_LENGTH = 100`) to ensure consistent input dimensions for the model.

### Model Architecture: T5-based Classifier

The classification model leverages the representational power of pre-trained Transformer models. Specifically, it employs the encoder component of the T5 (Text-To-Text Transfer Transformer) architecture [2], using the `google-t5/t5-small` variant pre-trained on a large text corpus.

The overall architecture (`T5EncoderClassifier`) comprises the following components:

1.  **Input Projection Layer (`custom_linear`):** Since the T5 encoder expects input embeddings of a specific hidden dimension (`d_model`, which is 512 for `t5-small`), a linear projection layer is introduced. This layer transforms the 42-dimensional keypoint feature vector for each frame into a 512-dimensional vector. This projection includes Layer Normalization and a GELU activation function to stabilize training and introduce non-linearity.
2.  **T5 Encoder:** The sequence of projected frame representations is fed into the pre-trained T5 encoder. The encoder's self-attention mechanisms process the sequence, capturing temporal dependencies and contextual information across the frames. An attention mask is provided to ensure the model ignores padded frames.
3.  **Feature Aggregation:** The output of the T5 encoder is a sequence of hidden states. For classification, we utilize the hidden state corresponding to the first element of the output sequence (analogous to the `[CLS]` token representation in BERT-like models). This vector is assumed to encapsulate a summary of the entire input sequence.
4.  **Classification Head:** The aggregated feature vector is passed through a dropout layer (with a rate of 0.2) for regularization, followed by a final linear layer that maps the T5 hidden dimension (512) to the number of output classes (10). The output of this layer represents the raw logits for each class.

During training, the Cross-Entropy Loss function is computed between the predicted logits and the true class labels. While the T5 encoder weights are loaded from the pre-trained checkpoint, all layers, including the encoder, are fine-tuned on the sign language classification task using the extracted keypoint data.

*(Consider adding references)*
*[1] Zhang, F., Bazarevsky, V., Vakunov, A., Tkachenka, A., Sung, G., Chang, C. L., & Grundmann, M. (2020). MediaPipe Hands: On-device Real-time Hand Tracking. arXiv preprint arXiv:2006.10214.*
*[2] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1-67.*

---

## Results and Discussion

The performance of the T5-based keypoint classification model was evaluated on a held-out test set, distinct from the data used for training and validation. The test set comprised 298 sequences, representing the 10 target sign language classes, using only the original (non-flipped) keypoint data.

### Overall Performance

The model was trained for 64 epochs using the Hugging Face `Trainer` with the AdamW optimizer, a learning rate of 5e-5, and a batch size of 8 (accumulated over 2 steps). The best model checkpoint, selected based on validation accuracy during training, was used for final evaluation.

The model achieved strong performance on the test set, demonstrating its effectiveness in classifying sign language words from normalized hand keypoints. The key performance metrics are summarized below:

*   **Accuracy:** 99.33%
*   **Weighted Precision:** 99.35%
*   **Weighted Recall:** 99.33%
*   **Weighted F1-Score:** 99.33%
*   **Validation Loss (at best epoch):** 0.0415

These high scores indicate that the model successfully learned discriminative features for the target classes and generalized well to unseen data.

### Classification Report Analysis

A detailed breakdown of performance per class is provided in the classification report (Table 1).

**Table 1: Classification Report for the Test Set**

| Class Name | Precision | Recall | F1-Score | Support |
| :--------- | :-------- | :----- | :------- | :------ |
| sleep      | 1.00      | 0.97   | 0.98     | 30      |
| silence    | 0.97      | 1.00   | 0.98     | 30      |
| love       | 1.00      | 1.00   | 1.00     | 30      |
| smoke      | 1.00      | 1.00   | 1.00     | 31      |
| support    | 1.00      | 1.00   | 1.00     | 30      |
| confused   | 1.00      | 1.00   | 1.00     | 30      |
| worried    | 0.97      | 1.00   | 0.98     | 30      |
| here       | 1.00      | 1.00   | 1.00     | 27      |
| greeting   | 1.00      | 0.97   | 0.98     | 30      |
| thanks     | 1.00      | 1.00   | 1.00     | 30      |
|            |           |        |          |         |
| **Accuracy** |           |        | **0.99** | **298** |
| Macro Avg  | 0.99      | 0.99   | 0.99     | 298     |
| Weighted Avg| 0.99      | 0.99   | 0.99     | 298     |

The report confirms the excellent performance across almost all classes. Precision, Recall, and F1-scores are predominantly 1.00 or very close (>= 0.97). Minor deviations occur for 'sleep' (one instance missed, recall 0.97), 'silence' (one instance incorrectly predicted as 'silence', precision 0.97), 'worried' (one instance incorrectly predicted as 'worried', precision 0.97), and 'greeting' (one instance missed, recall 0.97). These minor errors have minimal impact on the high weighted average scores, reflecting the model's overall robustness for this 10-class task.

### Confusion Matrix Analysis

The confusion matrix (Figure Y - *referencing the plot generated*) provides a visual representation of class-wise predictions versus true labels. The matrix exhibits a strong diagonal concentration, indicating that the vast majority of test sequences were assigned to their correct class.

The off-diagonal entries highlight the specific misclassifications identified in the classification report:
*   One instance of 'sleep' was misclassified as 'silence'.
*   One instance of 'greeting' was misclassified as 'worried'.

These constitute the only errors made by the model on the 298 test samples. The lack of widespread confusion between classes suggests that the extracted keypoint features, combined with the T5 encoder's temporal modeling, effectively capture the unique motion patterns of each sign.

### Training Dynamics Analysis

The training history plot (Figure Z - *referencing the plot generated*) illustrates the model's learning dynamics over the 64 epochs.

*   **Loss Curves:** Both the training loss and validation loss show a rapid decrease in the initial epochs, followed by convergence towards very low values. The validation loss closely tracks the training loss and remains stable without significant increases, suggesting that the model did not suffer from substantial overfitting, likely aided by the dropout regularization and the data augmentation strategy (horizontal flipping) applied to the training set.
*   **Metric Curves:** Validation accuracy and the weighted F1-score demonstrate a corresponding rapid increase, quickly reaching values near 1.0 and plateauing there. This indicates that the model achieved high performance early in the training process and maintained this performance, confirming stable learning and good generalization to the validation data (which mirrors the test set structure).

In conclusion, the evaluation results demonstrate that the proposed approach, combining MediaPipe keypoint extraction, normalization, flip augmentation, and a fine-tuned T5 encoder, yields a highly accurate and robust model for classifying the selected set of 10 sign language words. The near-perfect performance on the test set highlights the potential of using pre-trained Transformer encoders for processing spatio-temporal skeletal data in action recognition tasks.
